---
title: Obtaining the Most Listened Tracks on Spotify with Apache Airflow 
description: >-
  In this tutorial, we will use Apache Airflow to orchestrate a data pipeline that retrieves the most listened-to tracks worldwide and saves them in a MongoDB cluster.
date: 2024-11-29 00:00:00 +0800
categories: [Python, Airflow, Docker]
tags: [Airflow, Spotify]
image:
  path: /assets/posts/2024-11-29/airflow.png
render_with_liquid: false
---

## Introduction

In this project, we will put into practice some of the most common skills in the data world. The goal is to automatically retrieve the top 50 most listened-to songs on Spotify worldwide on a daily basis and store this information in a non-relational database, in this case, MongoDB.

### Pre-install

Our entire project will run on a pre-configured Docker image with Apache Airflow and MongoDB dependencies. 

You can check out a brief tutorial on how to install Airflow in the [previous blog post](https://lucasbral.github.io/posts/airflow/), which provides a more detailed explanation.

In our case, we will use the same Docker image from the tutorial with just one environment modification, as we will need to install the MongoDB dependency for Airflow. 

For more details, check the official [docs](https://www.mongodb.com/developer/products/mongodb/mongodb-apache-airflow/).

If they are not installed on your system, you can follow the tutorial available at [Docker](https://docs.docker.com/install/#supported-platforms) and [Docker Compose](https://docs.docker.com/compose/install/)

Remember to Add your user to the `docker` group by using a terminal to run:

```bash
sudo usermod -aG docker $USER
```
Sign out and back in again so your changes take effect

On the [official Apache Airflow page](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html), we find the link to download the latest version of Docker Compose. As of the time this post is being written, the most recent version is `2.10.3`, and we will be installing this version.

In a directory, create a `docker-compose.yaml` file with the following configurations:

```docker
x-airflow-common:
  &airflow-common
  build: ./airflow
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:
```

After that we will create the folders that will be shared between your computer and the container:

- `./dags`:  DAG directory.
- `./logs`: Will kepp logs of task executions and the scheduler.
- `./plugins`: Plugins directory.
- `./config`: Configuration files will be stored here.
- `./airflow`: Dockerfile directory.

On Linux, the quick-start setup needs to know your host user ID and requires the group ID to be set to 0. Otherwise, the files created in `dags`, `logs`, and `plugins` will be owned by the root user. You must ensure that these configurations are set correctly in your `docker-compose` setup.

```bash
mkdir -p ./dags ./logs ./plugins ./config ./airflow
echo -e "AIRFLOW_UID=$(id -u)" > .env
```
After that, create a `Dockerfile` in the `/airflow` folder with the following configurations:

```docker
FROM apache/airflow:2.10.3
# add your dependencies here:
RUN pip install apache-airflow-providers-mongo
```

The Apache Airflow instance can now be initialized using :

```bash
docker compose up --build airflow-init
```

Now you can start all services:

```bash
docker compose up
```

Airflow will be available at `0.0.0.0:8080`, and to log in, simply use `airflow` as both the username and password, as this is the default configuration.

## Spotify API

To obtain data from Spotify, it is necessary to create an account and also create an app within the Spotify API Dashboard under the Developer area. Here is a link to the API documentation with a brief description: [docs](https://developer.spotify.com/documentation/web-api).

Basically, we will need two pieces of information: the `client_id` and `client_secret`. With these two keys, it is possible to generate an `access_token` ([docs](https://developer.spotify.com/documentation/web-api/concepts/access-token)).

With the access token, it is possible to communicate with the Spotify API and retrieve a variety of information.

> The `access_token` is valid for only 1 hour, so we have the option to either refresh it or request a new one. In my case, I chose to request a new token each time, as the pipeline is scheduled to run only once per day.
{: .prompt-tip }

## Setting up Apache Airflow

Airflow has a very useful feature to define variables and secrets that can be used in DAGs and Tasks.

We will use it to store the Spotify API information.

To do this, on the front end, click on **Admin** > **Variables** > **add new record**

![desktop View](assets/posts/2024-11-29/1.png)
_add client_id and client_secret_

Add the `client_id` and `client_secret` information and click **Save**.

## Configuring Mongo DB

Before anything, it is necessary to create an account on [MongoDB](https://www.mongodb.com/) and set up a cluster. For this project, we will use the Free Tier.

![desktop View](assets/posts/2024-11-29/2.png)
_Mongo DB homepage_

Click on Database Access > add new database user

Use the authentication method with a password, define the user's role.

And make sure to store the password in a secure place as it will be needed later.

Click on Network Acess Allow IP access to the port.

![desktop View](assets/posts/2024-11-29/3.png)
_Acess Network_

It is also necessary to create a new database and collection within the cluster. This is where the music information will be saved. In my case, the database "spotify" was created in the "currency collection" collection.

![desktop View](assets/posts/2024-11-29/4.png)
_Creating Database and Collection_

## Creating a connection to MongoDB in Airflow
Agora que o MongoDB está devidamente configurado, é necessário conectá-lo no Airflow. Para isso, basta ir na aba **Admin > Connections** > **add new record** .

And set it up as shown below:

Set up the connection as shown below:

- **Conn Id:** mongo
- **Conn Type:** MongoDB
- **Host:** cluster0.mtfak.mongodb.net
- **Login:** myuser (set before on the MongoDB webpage)
- **Password:** mypass (set before on the MongoDB webpage)
- **Port:** empty
- **Extra:** `{"srv": true}`

Click on Save.

## Airflow DAG

Our pipeline will have 3 main parts:

1. Obtain the access_token from the Spotify API.
2. With the access_token in hand, make the request to the Spotify Playlist API to get the information of the 50 most played songs of the day (from the Top 50 Global playlist).
3. Save the JSON object in MongoDB.

### 1 - Get Acess Token

The get_token task retrieves the access token from the Spotify API by sending a request with the client credentials. It encodes the client ID and secret in base64, makes a POST request to Spotify's token endpoint, and returns the access token as an XCom.

```python
    @task
    def get_token():
        # get credentials
        client_id = Variable.get("client_id")
        client_secret = Variable.get("client_secret")

        auth_string = f"{client_id}:{client_secret}"
        auth_bytes = auth_string.encode('utf-8')
        auth_base64 = base64.b64encode(auth_bytes).decode('utf-8')
        # make request
        url = 'https://accounts.spotify.com/api/token'
        headers = {
            'Authorization': f'Basic {auth_base64}'
        }
        data = {
            'grant_type': 'client_credentials'
        }
        response = requests.post(url, headers=headers, data=data)
        print(response.json().get('access_token'))
        #transform token in a xcom
        return response.json().get('access_token')
```
### 2 - Obtain Playlist Songs

The get_playlist task retrieves the top 50 tracks from a specific Spotify playlist using the provided access token. It sends a GET request to the Spotify API and returns the playlist data in JSON format if the request is successful.

```python
    @task
    def get_playlist(token):
        url = 'https://api.spotify.com/v1/playlists/5FN6Ego7eLX6zHuCMovIR2/tracks?limit=50&offset=0'
        headers = {
        'Authorization': f'Bearer {token}'
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json()
```

### 3 - Ingest Data on Mongo DB

The load_data_to_mongo_db task connects to a MongoDB database using the MongoHook, inserts the result (with a timestamp) into the currency_collection of the spotify database, and logs the connection details.

```python
    @task
    def load_data_to_mongo_db(result):
        hook = MongoHook(conn_id='mongo')
        client = hook.get_conn()
        db = (
        client.spotify
        )
        currency_collection = db.currency_collection
        print(f"connected to MongoDB - {client.server_info()}")
        current_datetime = pendulum.now().to_iso8601_string()
        result["datetime"] = current_datetime
        currency_collection.insert_one(result)
```

Here is our complete DAG:

```python
from airflow.decorators import dag, task
import pendulum
from pendulum import datetime
from airflow.models import Variable
from airflow.providers.mongo.hooks.mongo import MongoHook
import json
import base64
import requests


@dag(
    start_date=datetime(2024, 11, 21),
    schedule="@daily",
    tags=["spotify_api"],
    catchup=False,
)

def pipe():
    @task
    def get_token():
        # get credentials
        client_id = Variable.get("client_id")
        client_secret = Variable.get("client_secret")

        # Codificando as credenciais em base64
        auth_string = f"{client_id}:{client_secret}"
        auth_bytes = auth_string.encode('utf-8')
        auth_base64 = base64.b64encode(auth_bytes).decode('utf-8')

        url = 'https://accounts.spotify.com/api/token'
        headers = {
            'Authorization': f'Basic {auth_base64}'
        }
        data = {
            'grant_type': 'client_credentials'
        }
        # Fazendo a solicitação POST
        response = requests.post(url, headers=headers, data=data)
        print(response.json().get('access_token'))
        return response.json().get('access_token')

    @task
    def get_playlist(token):
        url = 'https://api.spotify.com/v1/playlists/5FN6Ego7eLX6zHuCMovIR2/tracks?limit=50&offset=0'
        headers = {
        'Authorization': f'Bearer {token}'
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json()
        

    @task
    def load_data_to_mongo_db(result):
        hook = MongoHook(conn_id='mongo')
        client = hook.get_conn()
        db = (
        client.spotify
        )
        currency_collection = db.currency_collection
        print(f"connected to MongoDB - {client.server_info()}")
        current_datetime = pendulum.now().to_iso8601_string()
        result["datetime"] = current_datetime
        currency_collection.insert_one(result)
    

    load_data_to_mongo_db(get_playlist(get_token()))

pipe()
```

> The `pipe.py` must be on the dags directory.
{: .prompt-tip }

## Observing the Jobs in the Airflow Frontend

Now that our DAG is configured, simply run it manually once for testing, and let Airflow handle the orchestration to execute the code daily.

![desktop View](assets/posts/2024-11-29/6.png)
_Airflow Jobs running_

![desktop View](assets/posts/2024-11-29/7.png)
_Graph View Job_

The code worked as expected, and the pipeline is successfully retrieving playlist information, orchestrated via Airflow!

In MongoDB, it is possible to see the data loads made:

![desktop View](assets/posts/2024-11-29/8.png)
_Graph View Job_

## Conclusions

This project successfully demonstrates the power of Apache Airflow in orchestrating a data pipeline to fetch the most played tracks on Spotify daily. By leveraging Spotify's API and MongoDB for storage, we were able to automate the process of retrieving and storing the data. The use of Docker and MongoDB integration with Airflow allowed for a smooth execution of tasks, ensuring that the pipeline runs automatically each day with minimal intervention. This solution is a great example of how data engineering tools can be used to automate data collection and storage processes, providing valuable insights in an efficient and scalable manner.

All the codes are available in the [GitHub Repo](https://github.com/lucasbral/spotify_pipeline/tree/main).
